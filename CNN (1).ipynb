{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import seaborn as sns\n",
        "import torch.autograd.profiler as profiler\n",
        "\n",
        "\n",
        "# Define transformations for the training and test datasets, including data augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset with increased batch size and data augmentation\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbMomY8n0YnG",
        "outputId": "3fff9a87-65c8-4ef2-a763-5916476612ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.fc1 = nn.Linear(256 * 4 * 4, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
        "        x = self.pool(nn.functional.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 256 * 4 * 4)\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the network, loss function, and optimizer with GPU acceleration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "weight_decay = 1e-4  # Regularization strength\n",
        "net = Net().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=weight_decay)\n",
        "\n",
        "# Define class names for CIFAR-10 dataset\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# Lists to store training and testing metrics\n",
        "train_times = []\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "num_epochs = 50\n",
        "\n",
        "# Training and testing loops\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    net.train()  # Set the model to training mode\n",
        "    start_time = time.time()  # Record the start time of the epoch\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # Zero the parameter gradients\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_predictions += (predicted == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_time = end_time - start_time\n",
        "    train_times.append(epoch_time)\n",
        "\n",
        "    avg_loss = running_loss / len(trainloader)\n",
        "    accuracy = correct_predictions / total_samples\n",
        "    train_losses.append(avg_loss)\n",
        "    train_accuracies.append(accuracy)\n",
        "\n",
        "\n",
        "    # Testing\n",
        "    net.eval()  # Set the model to evaluation mode\n",
        "    test_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "    sample_images = []\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "    misclassified_images = []\n",
        "    misclassified_true_labels = []\n",
        "    misclassified_predicted_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        all_true_labels = []\n",
        "        all_predicted_labels = []\n",
        "        for i, data in enumerate(testloader, 0):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "            misclassified_indices = (predicted != labels).nonzero()[:, 0]\n",
        "\n",
        "            all_true_labels.extend(labels.cpu().numpy())\n",
        "            all_predicted_labels.extend(predicted.cpu().numpy())\n",
        "\n",
        "            # Store sample images, true labels, and predicted labels for a few examples\n",
        "            for j in range(len(labels)):\n",
        "                if len(sample_images) < 20:  # Store only 5 sample images\n",
        "                    sample_images.append(inputs[j].cpu())\n",
        "                    true_labels.append(class_names[labels[j].cpu().item()])  # Get class name instead of label number\n",
        "                    predicted_labels.append(class_names[predicted[j].cpu().item()])  # Get class name instead of label number\n",
        "\n",
        "            for idx in misclassified_indices:\n",
        "                misclassified_images.append(inputs[idx].cpu())\n",
        "                misclassified_true_labels.append(class_names[labels[idx].cpu().item()])\n",
        "                misclassified_predicted_labels.append(class_names[predicted[idx].cpu().item()])\n",
        "\n",
        "\n",
        "    avg_test_loss = test_loss / len(testloader)\n",
        "    test_accuracy = correct_predictions / total_samples\n",
        "    test_losses.append(avg_test_loss)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "\n",
        "    # Print training statistics including average time for the current epoch\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
        "          f'Training Loss: {avg_loss:.4f}, Training Accuracy: {accuracy * 100:.2f}%, '\n",
        "          f'Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "\n",
        "\n",
        "# Calculate the total training time\n",
        "total_training_time = sum(train_times)\n",
        "\n",
        "# Calculate the average time spent on each epoch\n",
        "average_time_per_epoch = total_training_time / num_epochs\n",
        "\n",
        "print(f'Total training time: {total_training_time:.2f} seconds')\n",
        "print(f'Average time spent on each epoch: {average_time_per_epoch:.2f} seconds')\n",
        "\n",
        "# Compute confusion matrix, accuracy, precision, and recall\n",
        "conf_matrix = confusion_matrix(all_true_labels, all_predicted_labels)\n",
        "accuracy = accuracy_score(all_true_labels, all_predicted_labels)\n",
        "precision = precision_score(all_true_labels, all_predicted_labels, average='weighted')\n",
        "recall = recall_score(all_true_labels, all_predicted_labels, average='weighted')\n",
        "f1_score = f1_score(all_true_labels, all_predicted_labels, average='weighted')\n",
        "\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'F1-Score: {f1_score:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lbVlF1Y0doi",
        "outputId": "45cd0919-6c17-45c4-e863-8a521f537e73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Training Loss: 1.4014, Training Accuracy: 49.06%, Test Loss: 1.0388, Test Accuracy: 62.04%\n",
            "Epoch [2/50], Training Loss: 0.9711, Training Accuracy: 65.54%, Test Loss: 0.8283, Test Accuracy: 71.04%\n",
            "Epoch [3/50], Training Loss: 0.7946, Training Accuracy: 72.01%, Test Loss: 0.7276, Test Accuracy: 74.67%\n",
            "Epoch [4/50], Training Loss: 0.6882, Training Accuracy: 75.84%, Test Loss: 0.7006, Test Accuracy: 76.03%\n",
            "Epoch [5/50], Training Loss: 0.6178, Training Accuracy: 78.56%, Test Loss: 0.6733, Test Accuracy: 77.31%\n",
            "Epoch [6/50], Training Loss: 0.5621, Training Accuracy: 80.34%, Test Loss: 0.6032, Test Accuracy: 79.32%\n",
            "Epoch [7/50], Training Loss: 0.5119, Training Accuracy: 82.14%, Test Loss: 0.5885, Test Accuracy: 79.66%\n",
            "Epoch [8/50], Training Loss: 0.4631, Training Accuracy: 83.76%, Test Loss: 0.5868, Test Accuracy: 80.29%\n",
            "Epoch [9/50], Training Loss: 0.4317, Training Accuracy: 84.92%, Test Loss: 0.5832, Test Accuracy: 80.70%\n",
            "Epoch [10/50], Training Loss: 0.3954, Training Accuracy: 86.19%, Test Loss: 0.6005, Test Accuracy: 80.75%\n",
            "Epoch [11/50], Training Loss: 0.3648, Training Accuracy: 87.27%, Test Loss: 0.5884, Test Accuracy: 81.15%\n",
            "Epoch [12/50], Training Loss: 0.3440, Training Accuracy: 87.95%, Test Loss: 0.5699, Test Accuracy: 81.38%\n",
            "Epoch [13/50], Training Loss: 0.3164, Training Accuracy: 88.89%, Test Loss: 0.5931, Test Accuracy: 81.67%\n",
            "Epoch [14/50], Training Loss: 0.2949, Training Accuracy: 89.71%, Test Loss: 0.5964, Test Accuracy: 81.01%\n",
            "Epoch [15/50], Training Loss: 0.2778, Training Accuracy: 90.22%, Test Loss: 0.6110, Test Accuracy: 81.52%\n",
            "Epoch [16/50], Training Loss: 0.2608, Training Accuracy: 90.74%, Test Loss: 0.5844, Test Accuracy: 82.20%\n",
            "Epoch [17/50], Training Loss: 0.2452, Training Accuracy: 91.43%, Test Loss: 0.6149, Test Accuracy: 81.78%\n",
            "Epoch [18/50], Training Loss: 0.2317, Training Accuracy: 91.85%, Test Loss: 0.5974, Test Accuracy: 81.74%\n",
            "Epoch [19/50], Training Loss: 0.2196, Training Accuracy: 92.32%, Test Loss: 0.6325, Test Accuracy: 81.72%\n",
            "Epoch [20/50], Training Loss: 0.2070, Training Accuracy: 92.70%, Test Loss: 0.6219, Test Accuracy: 82.35%\n",
            "Epoch [21/50], Training Loss: 0.2033, Training Accuracy: 92.82%, Test Loss: 0.6120, Test Accuracy: 82.90%\n",
            "Epoch [22/50], Training Loss: 0.1953, Training Accuracy: 93.20%, Test Loss: 0.6166, Test Accuracy: 82.10%\n",
            "Epoch [23/50], Training Loss: 0.1812, Training Accuracy: 93.64%, Test Loss: 0.6550, Test Accuracy: 82.11%\n",
            "Epoch [24/50], Training Loss: 0.1864, Training Accuracy: 93.42%, Test Loss: 0.6485, Test Accuracy: 82.09%\n",
            "Epoch [25/50], Training Loss: 0.1747, Training Accuracy: 93.82%, Test Loss: 0.6290, Test Accuracy: 83.00%\n",
            "Epoch [26/50], Training Loss: 0.1665, Training Accuracy: 94.05%, Test Loss: 0.6715, Test Accuracy: 82.13%\n",
            "Epoch [27/50], Training Loss: 0.1610, Training Accuracy: 94.41%, Test Loss: 0.6859, Test Accuracy: 82.49%\n",
            "Epoch [28/50], Training Loss: 0.1612, Training Accuracy: 94.41%, Test Loss: 0.6704, Test Accuracy: 81.94%\n",
            "Epoch [29/50], Training Loss: 0.1593, Training Accuracy: 94.49%, Test Loss: 0.6625, Test Accuracy: 82.35%\n",
            "Epoch [30/50], Training Loss: 0.1524, Training Accuracy: 94.65%, Test Loss: 0.7180, Test Accuracy: 81.58%\n",
            "Epoch [31/50], Training Loss: 0.1535, Training Accuracy: 94.62%, Test Loss: 0.6865, Test Accuracy: 82.42%\n",
            "Epoch [32/50], Training Loss: 0.1489, Training Accuracy: 94.76%, Test Loss: 0.6889, Test Accuracy: 82.21%\n",
            "Epoch [33/50], Training Loss: 0.1435, Training Accuracy: 95.01%, Test Loss: 0.6729, Test Accuracy: 82.06%\n",
            "Epoch [34/50], Training Loss: 0.1470, Training Accuracy: 94.82%, Test Loss: 0.7427, Test Accuracy: 80.65%\n",
            "Epoch [35/50], Training Loss: 0.1456, Training Accuracy: 94.90%, Test Loss: 0.7108, Test Accuracy: 82.22%\n",
            "Epoch [36/50], Training Loss: 0.1385, Training Accuracy: 95.25%, Test Loss: 0.6890, Test Accuracy: 82.44%\n",
            "Epoch [37/50], Training Loss: 0.1351, Training Accuracy: 95.24%, Test Loss: 0.7130, Test Accuracy: 82.74%\n",
            "Epoch [38/50], Training Loss: 0.1320, Training Accuracy: 95.34%, Test Loss: 0.7164, Test Accuracy: 82.17%\n",
            "Epoch [39/50], Training Loss: 0.1353, Training Accuracy: 95.21%, Test Loss: 0.7042, Test Accuracy: 82.71%\n",
            "Epoch [40/50], Training Loss: 0.1315, Training Accuracy: 95.39%, Test Loss: 0.7418, Test Accuracy: 82.31%\n",
            "Epoch [41/50], Training Loss: 0.1276, Training Accuracy: 95.51%, Test Loss: 0.6996, Test Accuracy: 83.04%\n",
            "Epoch [42/50], Training Loss: 0.1302, Training Accuracy: 95.46%, Test Loss: 0.7087, Test Accuracy: 82.66%\n",
            "Epoch [43/50], Training Loss: 0.1265, Training Accuracy: 95.64%, Test Loss: 0.7154, Test Accuracy: 82.71%\n",
            "Epoch [44/50], Training Loss: 0.1292, Training Accuracy: 95.46%, Test Loss: 0.6854, Test Accuracy: 82.16%\n",
            "Epoch [45/50], Training Loss: 0.1303, Training Accuracy: 95.43%, Test Loss: 0.6789, Test Accuracy: 82.81%\n",
            "Epoch [46/50], Training Loss: 0.1225, Training Accuracy: 95.78%, Test Loss: 0.7221, Test Accuracy: 82.78%\n",
            "Epoch [47/50], Training Loss: 0.1220, Training Accuracy: 95.81%, Test Loss: 0.7456, Test Accuracy: 82.46%\n",
            "Epoch [48/50], Training Loss: 0.1251, Training Accuracy: 95.62%, Test Loss: 0.7082, Test Accuracy: 82.97%\n",
            "Epoch [49/50], Training Loss: 0.1183, Training Accuracy: 95.87%, Test Loss: 0.7351, Test Accuracy: 82.27%\n",
            "Epoch [50/50], Training Loss: 0.1236, Training Accuracy: 95.70%, Test Loss: 0.7129, Test Accuracy: 83.05%\n",
            "Total training time: 1293.09 seconds\n",
            "Average time spent on each epoch: 25.86 seconds\n",
            "Accuracy: 83.05%\n",
            "Precision: 0.83\n",
            "Recall: 0.83\n",
            "F1-Score: 0.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from prettytable import PrettyTable\n",
        "\n",
        "# Display sample images along with true and predicted labels\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.suptitle('Sample Test Result', weight=\"bold\")\n",
        "for i in range(len(sample_images)):\n",
        "    plt.subplot(4, 5, i + 1)\n",
        "    image = sample_images[i].permute(1, 2, 0) * 0.5 + 0.5  # Unnormalize the image\n",
        "    plt.imshow(image.numpy())\n",
        "    plt.title(f'True: {true_labels[i]}\\nPredicted: {predicted_labels[i]}')\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display some correctly classified images along with true and predicted labels\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.suptitle('Misclassified Samples', weight=\"bold\")\n",
        "for i in range(20):\n",
        "    plt.subplot(4, 5, i + 1)  # 4 rows, 5 columns for 20 images\n",
        "    image = misclassified_images[i].permute(1, 2, 0) * 0.5 + 0.5  # Unnormalize the image\n",
        "    plt.imshow(image.numpy())\n",
        "    plt.title(f'True: {misclassified_true_labels[i]}\\nPredicted: {misclassified_predicted_labels[i]}')\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plotting training and testing metrics\n",
        "plt.figure(figsize=(16, 6))\n",
        "\n",
        "# Plot training time\n",
        "plt.subplot(1, 4, 1)\n",
        "plt.plot(train_times, marker='o', linestyle='-', color='b')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Time (seconds)')\n",
        "plt.title('Training Time for Each Epoch')\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot training and testing loss curves\n",
        "plt.subplot(1, 4, 2)\n",
        "plt.plot(train_losses, marker='o', linestyle='-', color='r', label='Training')\n",
        "plt.plot(test_losses, marker='o', linestyle='-', color='g', label='Test')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Test Loss Curves')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot training and testing accuracy curves\n",
        "plt.subplot(1, 4, 3)\n",
        "plt.plot(train_accuracies, marker='o', linestyle='-', color='r', label='Training')\n",
        "plt.plot(test_accuracies, marker='o', linestyle='-', color='g', label='Test')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Test Accuracy Curves')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jKhr6GfB5hM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from prettytable import PrettyTable\n",
        "\n",
        "# Create a colored table for accuracy, precision, and recall\n",
        "table = PrettyTable(['Metric', 'Value'])\n",
        "table.title = 'Model Metrics'\n",
        "table.align['Metric'] = 'l'\n",
        "table.align['Value'] = 'r'\n",
        "table.add_row(['\\033[92mAccuracy\\033[0m', f'{accuracy * 100:.2f}%'])\n",
        "table.add_row(['\\033[91mPrecision\\033[0m', f'{precision:.2f}'])\n",
        "table.add_row(['\\033[94mRecall\\033[0m', f'{recall:.2f}'])\n",
        "\n",
        "# Print the colored table\n",
        "print(table)"
      ],
      "metadata": {
        "id": "UM_3QZx-EEnv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}